{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a707689",
   "metadata": {},
   "source": [
    "# News Classifier with StreamLit App\n",
    "Build a news classifier model using news article from Huffing Post from <a href=\"https://www.kaggle.com/datasets/rmisra/news-category-dataset\">Kaggle News Category Dataset</a> across 42 categories. </br>\n",
    "We will fine-tune Google bert cased model for news classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f65e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import pickle\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as r\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from json import load, dump, loads, dumps\n",
    "from joblib import hash\n",
    "import torch\n",
    "\n",
    "# Configure matplotlib\n",
    "mpl.rcParams['figure.figsize'] = (12, 6)\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['axes.labelsize'] = 'large'\n",
    "mpl.rcParams['xtick.labelsize'] = 'medium'\n",
    "mpl.rcParams['ytick.labelsize'] = 'medium'\n",
    "warnings.simplefilter('ignore')\n",
    "mpl.style.use('ggplot')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613cf209",
   "metadata": {},
   "source": [
    "## Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652247d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "from io import TextIOWrapper\n",
    "\n",
    "def load_json_from_zip(zip_path: str, zip_filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Opens a zip file, reads a JSON file inside, and loads it into a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        zip_path (str): Path to the .zip file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame created from the JSON file.\n",
    "    \"\"\"\n",
    "    path_to_zip_file = zip_path + zip_filename\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(zip_path) \n",
    "        \n",
    "        # Find all files with .json extension\n",
    "        json_filenames = [name for name in zip_ref.namelist() if name.endswith('.json')]\n",
    "        json_filename = zip_path+json_filenames[0]  # assumes only one JSON file\n",
    "        df = pd.read_json(json_filename, lines=True, encoding='utf-8')\n",
    "        os.system(f'rm {json_filename}')  # remove the extracted JSON file\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Example usage:\n",
    "df = load_json_from_zip(zip_path=\"../data/\",zip_filename=\"news-category-dataset.zip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017e25b",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory analysis of the dataset\n",
    "df.category.value_counts().plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Distribution of News Categories')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "df.columns\n",
    "df[\"text\"] = df[\"headline\"] + \"\\n\" + df[\"short_description\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae497bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into a huggingface dataset\n",
    "from datasets import Dataset, ClassLabel\n",
    "dataset = Dataset.from_pandas(df[['text', 'category']])\n",
    "\n",
    "# Obtain label as ClassLabel column from category column in dataset and add it to the dataset\n",
    "# This will convert the 'category' column into a ClassLabel type, which is useful for classification tasks. \n",
    "label_list = dataset.unique('category')\n",
    "num_labels = len(label_list)\n",
    "class_label = ClassLabel(num_classes=num_labels, names=label_list)\n",
    "dataset = dataset.cast_column('category', class_label)\n",
    "dataset = dataset.rename_column('category', 'label')  # Rename column for consistency\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Split the train dataset into train and validation sets\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column='label')\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']\n",
    "\n",
    "# Print the number of samples in each dataset\n",
    "print(f\"Train dataset size: {len(train_dataset)}\") \n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 256  # Maximum length for tokenization, can be adjusted based on model requirements\n",
    "\n",
    "# Model id to load the tokenizer\n",
    "model_id = \"google-bert/bert-base-uncased\"  # You can change this to any other model id\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    " \n",
    "# Tokenize helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    " \n",
    "# Tokenize dataset\n",
    "tokenized_dataset = train_val_split.map(tokenize, batched=True, remove_columns=[\"text\"], batch_size=32) # type: ignore\n",
    "tokenized_dataset[\"train\"].features.keys()\n",
    "\n",
    "# Test tokenize dataset\n",
    "test_tokenized_dataset = test_dataset.map(tokenize, batched=True, remove_columns=[\"text\"], batch_size=32) # type: ignore\n",
    "test_tokenized_dataset.features.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f84d3",
   "metadata": {},
   "source": [
    "## Prepare label 2 id and id 2 label and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc114505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification \n",
    " \n",
    "# Prepare model labels - useful for inference\n",
    "labels = tokenized_dataset[\"train\"].features[\"label\"].names\n",
    "print(labels)\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "# Download the model from huggingface.co/models\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae0909",
   "metadata": {},
   "source": [
    "## Evaluate Model for F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3913f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    " \n",
    "# Metric helper method\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    score = f1_score(\n",
    "            labels, predictions, labels=labels, pos_label=1, average=\"weighted\"\n",
    "        )\n",
    "    return {\"f1\": float(score) if score == 1 else score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0aa1a",
   "metadata": {},
   "source": [
    "## Fine-tune the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from transformers import Trainer, TrainingArguments\n",
    " \n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"../models/bert-uncased-news-classifier\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=10,\n",
    "    bf16=True, # bfloat16 training\n",
    "    optim=\"adamw_torch_fused\", # improved optimizer \n",
    "    # logging & evaluation strategies\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1, # keep only the latest checkpoint\n",
    "    load_best_model_at_end=True,\n",
    "    # evaluation metrics\n",
    "    metric_for_best_model=\"f1\",\n",
    "    # push to hub parameters\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# # Create a Trainer instance\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset[\"train\"],\n",
    "#     eval_dataset=tokenized_dataset[\"test\"],\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffe990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved huggingface model and test on the test dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model from the saved directory\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../models/bert-uncased-news-classifier\", local_files_only=True)\n",
    "\n",
    "# Create a pipeline for text classification\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device='cuda:0')\n",
    "\n",
    "# Test the classifier on the test dataset\n",
    "test_predictions = classifier(test_dataset[\"text\"], truncation=True, padding=True, max_length=max_length)\n",
    "# Print the first 5 predictions\n",
    "for i in range(5):\n",
    "    print(f\"Text: {test_dataset['text'][i]}\")\n",
    "    print(f\"Predicted label: {test_predictions[i]['label']}, Score: {test_predictions[i]['score']:.4f}\\n\") # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weighted F1 score on the test predictions\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "test_labels = test_dataset[\"label\"] \n",
    "f1 = f1_score(test_labels, [int(label2id[pred['label']]) for pred in test_predictions], average='weighted') # type: ignore\n",
    "print(f\"Weighted F1 Score on Test Set: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42658c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
